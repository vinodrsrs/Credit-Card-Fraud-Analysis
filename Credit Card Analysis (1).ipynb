{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set_style(\"darkgrid\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data  = pd.read_csv(\"/kaggle/input/creditcardfraud/creditcard.csv\")\ndata  = pd.DataFrame(data)\nprint(\"DataFrame shape :\",data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# missing_values in dataframe\ndata.isnull().sum()/len(data)*100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"fraud_amount = data[data.Class == 1][\"Amount\"]\nfraud_amount = fraud_amount.astype(int)\nfraud_amount.hist(color = \"r\",alpha = 0.6)\nplt.xlabel(\"Fraud Amount\")\nplt.ylabel(\"Frequency\")\nprint(\"Highest Fruad amount was :\",max(fraud_amount))\nprint(\"Least   Fruad amount was :\",min(fraud_amount))\n# i have no idea  how they  , considered amount 0 as fruad transcation .!! Because there is no transcation at all\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# most of the fraud amount was below 1000\nfraud_amount.hist(color = \"b\",alpha = 0.3,bins = [1,1000,2000])\nplt.xlabel(\"Fraud Amount\")\nplt.ylabel(\"Frequency\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# our Data seems imbalanced \nsns.countplot(data[\"Class\"],color = \"orange\")\nprint(data[\"Class\"].value_counts())\nprint(\"=\"*60)\nprint(\"Percentage of class values :\")\nprint(data[\"Class\"].value_counts()/len(data)*100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[\"Amount\"].hist(color = \"y\",bins = [0,500,1500,2000])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\ndata[\"scaled_amount\"] = sc.fit_transform(np.array(data[\"Amount\"]).reshape(-1,1))\ndata.drop([\"Amount\",\"Time\"],axis = 1,inplace = True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report,confusion_matrix,accuracy_score,roc_auc_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = data.loc[:,data.columns != \"Class\"]\ny = data.loc[:,\"Class\"]\nprint(\"Features shape :\",x.shape)\nprint(\"Dependent Variable shape \",y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_set(ytrain,train_pred):\n    print(\"confusion matrix for train set : \")\n    cm = confusion_matrix(ytrain,train_pred)\n    print(cm)\n    print(\"--\"*40)\n    print(\"False positive rate :\",(cm[1][0]/(cm[1][0]+cm[1][1]))*100)  # FPR = FP/FP +TN\n    print(\"\\n\")\n    print(cm[1][0] ,\"out of\",(cm[1][0]+cm[1][1]),\"fraud transaction instances were classified as not a fraudulent transactions \\n\")\n    print(\"--\"*40)\n    print(\"False Negative rate :\",(cm[0][1]/(cm[0][1]+cm[0][0]))*100) # FNR = FN + (FN + TP )\n    print(\"\\n\")\n    print(cm[0][1],\"out of \",(cm[0][1]+cm[0][0]),\"non fraudulent transaction instances were classified as  a fraudulent transactions\")\n    print(\"--\"*40)\n    print(classification_report(ytrain,train_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def test_set(ytest,test_pred):\n    print(\"confusion matrix for test set : \")\n    cm = confusion_matrix(ytest,test_pred)\n    print(cm)\n    print(\"--\"*40)\n    print(\"False positive rate :\",(cm[1][0]/(cm[1][0]+cm[1][1]))*100)  # FPR = FP/FP +TN\n    print(\"\\n\")\n    print(cm[1][0] ,\"out of\",(cm[1][0]+cm[1][1]),\"fraud transaction instances were classified as not a fraudulent transactions \\n\")\n    print(\"--\"*40)\n    print(\"False Negative rate :\",(cm[0][1]/(cm[0][1]+cm[0][0]))*100) # FNR = FN + (FN + TP )\n    print(\"\\n\")\n    print(cm[0][1],\"out of \",(cm[0][1]+cm[0][0]),\"non fraudulent transaction instances were classified as  a fraudulent transactions\")\n    print(\"--\"*40)\n    print(classification_report(ytest,test_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def under_sample(data):\n    under_sample_zero = data[data.Class == 0].iloc[:,:]\n    under_sample_zero = under_sample_zero.sample(data.Class.value_counts()[1])\n    under_sample_one  = data[data.Class == 1].iloc[:,:]\n    under_sampled_data = pd.concat([under_sample_zero,under_sample_one],axis = 0)\n    x = under_sampled_data.loc[:,under_sampled_data.columns != \"Class\"]\n    y = under_sampled_data.loc[:,\"Class\"]\n    return us_x,us_y\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now we have equal number of 1's and 0's ,i.e we have balanced data !\n\nprint(\"under_sampled_data size      : \",under_sampled_data.shape)\nprint(\"sample size where class is 0 :\",under_sample_zero.shape)\nprint(\"sample size where class is 1 :\",under_sample_one.shape)\nprint(\"Features shape :\",x.shape)\nprint(\"Dependent Variable shape \",y.shape)\nsns.countplot(under_sampled_data.Class)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. #  Let's build our model without treating imbalanced data"},{"metadata":{},"cell_type":"markdown","source":"# Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nlr_model = LogisticRegression()\nxtrain,xtest,ytrain,ytest = train_test_split(x,y,test_size = 0.4,random_state = 1)\nlr_model.fit(xtrain,ytrain)\n#train_set\ntrain_pred = lr_model.predict(xtrain)\n#test_set\ntest_pred  = lr_model.predict(xtest)\nprint(\"Accuracy for Training set : \",accuracy_score(ytrain,train_pred))\nprint(\"Accuracy for Testing set  : \",accuracy_score(ytest,test_pred))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_set(ytrain,train_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 33% of  fraud transaction instances were classified as not a fraudulent transactions"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_set(ytest,test_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##  40% fraud transaction instances were classified as not a fraudulent transactions ,that's really bad"},{"metadata":{},"cell_type":"markdown","source":"## *our goal is to reduce false positive rate* ,\nNote :  again it's entirely depends on the business requirement \n"},{"metadata":{},"cell_type":"markdown","source":"# Let's use  Under Sampling technique for handling imbalanced data"},{"metadata":{},"cell_type":"markdown","source":"# Now using Logistic Regression after allowing Under_sampling"},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_model = LogisticRegression()\nus_x,us_y = under_sample(data)\nxtrain,xtest,ytrain,ytest = train_test_split(us_x,us_y,test_size = 0.4,random_state = 1)\nlr_model.fit(xtrain,ytrain)\n#train_set\ntrain_pred = lr_model.predict(xtrain)\n#test_set\ntest_pred  = lr_model.predict(xtest)\nprint(\"Accuracy for Training set : \",accuracy_score(ytrain,train_pred))\nprint(\"Accuracy for Testing set  : \",accuracy_score(ytest,test_pred))\nroc_auc_logistic = roc_auc_score(ytest,test_pred)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_set(ytrain,train_pred)\n# on applying undersampling technique we got !\n# Decrease in False positive rate ,\n# Increase in precision,recall,f1-score ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_set(ytest,test_pred)\n# on applying undersampling technique we got !\n# Decrease in False positive rate ,\n# Increase in precision,recall,f1-score ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Random Forest Classifier "},{"metadata":{},"cell_type":"markdown","source":"## let's analyze effect of  random forest without handling imbalanced data.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nrf_model = RandomForestClassifier(n_estimators=20,max_depth = 10,min_samples_split = 20)\nxtrain,xtest,ytrain,ytest = train_test_split(x,y,test_size = 0.4,random_state = 1)\nrf_model.fit(xtrain,ytrain)\n#train_set\ntrain_pred = rf_model.predict(xtrain)\n#test_set\ntest_pred  = rf_model.predict(xtest)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Accuracy for Training set : \",accuracy_score(ytrain,train_pred))\nprint(\"Accuracy for Testing set  : \",accuracy_score(ytest,test_pred)) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_set(ytrain,train_pred)\n# 57 out of 306 fraud transaction instances were classified as non a fraudulent transactions ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_set(ytest,test_pred)\n# 51 out of 186 fraud transaction instances were classified as not a fraudulent transactions\n# 25% of fradulent transaction were classified as non fraudulent transactions","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# let's use UnderSampling technique for Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"us_x,us_y = under_sample(data)\nrf_model = RandomForestClassifier(n_estimators=20,max_depth = 10,min_samples_split = 20)\nxtrain,xtest,ytrain,ytest = train_test_split(us_x,us_y,test_size = 0.4,random_state = 1)\nrf_model.fit(xtrain,ytrain)\n#train_set\ntrain_pred = rf_model.predict(xtrain)\n#test_set\ntest_pred  = rf_model.predict(xtest)\nroc_auc_random = roc_auc_score(ytest,test_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_set(ytrain,train_pred)\n# on applying undersampling technique we got !\n# Decrease in False positive rate ,\n# Increase in precision,recall,f1-score ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_set(ytest,test_pred)\n# on applying undersampling technique we got !\n# Decrease in False positive rate ,\n# Increase in precision,recall,f1-score ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Let's Calculate roc_auc_score in order to find out which algorithm out of (Logistic Reg v/s Random Forest) performed well on applying Undersampling Technique . "},{"metadata":{"trusted":true},"cell_type":"code","source":"# for test_set's only (validation set)\nprint(\"ROC_AUC_SCORE FOR LOGISTIC REG IS  : \",roc_auc_logistic)\nprint(\"ROC_AUC_SCORE FOR Random Forest IS : \",roc_auc_random)\n# both algo's are working equally ,","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Let's See Handling Imbalanced Data with Oversampling Technique (SMOTE).\nhttps://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.over_sampling.SMOTE.html"},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\nsm = SMOTE(random_state = 1)\nover_sam_x,over_sam_y = sm.fit_sample(x,y)\nprint(\"Feature size   after oversampling :\",over_sam_x.shape)\nprint(\"Dependent size after oversampling :\",over_sam_y.shape)\nsns.countplot(y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# over sampling technique using Logistic Regression "},{"metadata":{"trusted":true},"cell_type":"code","source":"\nxtrain,xtest,ytrain,ytest = train_test_split(over_sam_x,over_sam_y,test_size = 0.4,random_state = 1)\nlr_model.fit(xtrain,ytrain)\n#train_set\ntrain_pred = lr_model.predict(xtrain)\n#test_set\ntest_pred  = lr_model.predict(xtest)\nprint(\"Accuracy for Training set : \",accuracy_score(ytrain,train_pred))\nprint(\"Accuracy for Testing set  : \",accuracy_score(ytest,test_pred))\nover_sam_roc_auc_logistic = roc_auc_score(ytest,test_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_set(ytrain,train_pred)\n# on applying undersampling technique we got !\n# Decrease in False positive rate ,\n# Increase in precision,recall,f1-score ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_set(ytest,test_pred)\n# on applying undersampling technique we got !\n# Decrease in False positive rate ,\n# Increase in precision,recall,f1-score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# over sampling technique Random Forest Algorithm"},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_model = RandomForestClassifier(n_estimators=20,max_depth = 10,min_samples_split = 20)\nxtrain,xtest,ytrain,ytest = train_test_split(over_sam_x,over_sam_y,test_size = 0.4,random_state = 1)\nrf_model.fit(xtrain,ytrain)\n#train_set\ntrain_pred = rf_model.predict(xtrain)\n#test_set\ntest_pred  = rf_model.predict(xtest)\nprint(\"Accuracy for Training set : \",accuracy_score(ytrain,train_pred))\nprint(\"Accuracy for Testing set  : \",accuracy_score(ytest,test_pred))\nover_sam_roc_auc_random_forest = roc_auc_score(ytest,test_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_set(ytrain,train_pred)\n# on applying undersampling technique we got !\n# Decrease in False positive rate ,\n# Increase in precision,recall,f1-score ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_set(ytest,test_pred)\n# on applying undersampling technique we got !\n# Decrease in False positive rate ,\n# Increase in precision,recall,f1-score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Let's Calculate roc_auc_score in order to find out which algorithm out of (Logistic Reg v/s Random Forest) performed well on applying OverSampling Technique . "},{"metadata":{"trusted":true},"cell_type":"code","source":"# for test_set's only \nprint(\"ROC_AUC_SCORE FOR LOGISTIC REG IS  : \",over_sam_roc_auc_logistic)\nprint(\"ROC_AUC_SCORE FOR Random Forest IS : \",over_sam_roc_auc_random_forest)\n#  Random Forest performs well compared to Logistic Regression.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Note (in this use case) : On applying Over Sampling technique in order to deal with Imbalanced Dataset provides better result compared to Under Sampling technique ( observe  Confusion Matrix of under v/s over techniques in order to understand )"},{"metadata":{},"cell_type":"markdown","source":"Thank you :)"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}